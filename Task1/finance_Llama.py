from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch

# æ¨¡å‹IDï¼ˆé‡‘èä¸“ç”¨Llamaï¼‰
model_id = "tarun7r/Finance-Llama-8B"

print("Loading Finance Llama model...")

# åŠ è½½æ¨¡å‹ï¼ˆå¸¦å†…å­˜ä¼˜åŒ–ï¼‰
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,  # åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
        device_map="auto",          # è‡ªåŠ¨åˆ†é…è®¾å¤‡
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    print("âœ“ Model loaded with FP16")
except Exception as e:
    print(f"FP16 loading failed: {e}")
    print("Fallback: loading on CPU...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="cpu",
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    print("âœ“ Model loaded on CPU")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åˆ›å»ºpipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer
)
print("âœ“ Pipeline created successfully!")


def chat():
    """äº¤äº’å¼èŠå¤©"""
    print("\n" + "="*60)
    print("Finance Llama Chatbot - Interactive Mode")
    print("="*60)
    print("Type 'quit' or 'exit' to stop\n")

    system_prompt = (
        "You are a highly knowledgeable finance chatbot. "
        "Your purpose is to provide accurate, insightful, and actionable financial advice to users."
    )

    while True:
        user_input = input("\nYou: ").strip()
        if user_input.lower() in ["quit", "exit"]:
            print("Assistant: Goodbye! ğŸ‘‹")
            break
        if not user_input:
            continue

        # æ„å»ºprompt
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input},
        ]
        prompt = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in messages])

        print("Assistant: (thinking...)")

        try:
            outputs = generator(
                prompt,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                num_beams=1,
                use_cache=True,
                max_new_tokens=50
            )

            generated_text = outputs[0]["generated_text"]
            response_start = generated_text.rfind("User:")
            if response_start != -1:
                response = generated_text[response_start:].split("Assistant:")[-1].strip()
            else:
                response = generated_text[len(prompt):].strip()

            print(f"Assistant: {response}")

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"Error: {e}")


if __name__ == "__main__":
    chat()
